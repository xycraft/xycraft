<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Xiaoying Yang</title>

  <!--
    - favicon
  -->
  <link rel="shortcut icon" href="./assets/images/icon.jpg">

  <!--
    - custom css link
  -->
  <link rel="stylesheet" href="./assets/css/style.css">

  <!--
    - google font link
  -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Poppins:wght@300;400;500;600&display=swap" rel="stylesheet">
</head>

<body>

  <!--
    - #MAIN
  -->

  <main>

    <!--
      - #SIDEBAR
    -->

    <aside class="sidebar" data-sidebar>

      <div class="sidebar-info">

        <figure class="profile-box">
          <img src="./assets/images/profile.jpg" alt="Xiaoying Yang" width="80">
        </figure>

        <div class="info-content">
          <h1 class="name" title="Xiaoying Yang">Xiaoying Yang</h1>

          <p class="title">
            HCI Researcher
          </p>
        </div>

        <button class="info_more-btn" data-sidebar-btn>
          <span>Contact</span>

          <ion-icon name="chevron-down"></ion-icon>
        </button>

      </div>

      <div class="sidebar-info_more">

        <div class="separator"></div>

        <ul class="contact-list">

          <li class="contact-item">
            <a href="mailto:xiaoyingy@ucla.edu" class="contact-link">
              <ion-icon name="mail-outline"></ion-icon>
            </a>
          </li>

          <li class="contact-item">
            <a href="https://scholar.google.com/citations?user=8h6LYxcAAAAJ&hl=en" class="contact-link"
              title="Google Scholar">
              <img src="./assets/images/google-scholar.svg" alt="Google Scholar" class="icon-img">
            </a>
          </li>


          <li class="contact-item">
            <a href="https://x.com/xiaoyingucla" class="contact-link" title="Twitter">
              <img src="./assets/images/logo-twitter.svg" alt="Twitter" class="icon-img">
            </a>
          </li>

          <li class="contact-item">
            <a href="https://www.linkedin.com/in/xyyang09/" class="contact-link" title="Linkedin">
              <img src="./assets/images/logo-linkedin.svg" alt="Linkedin" class="icon-img">
            </a>
          </li>

        </ul>

      </div>

    </aside>





    <!--
      - #main-content
    -->

    <div class="main-content">

      <!--
        - #NAVBAR
      -->

      <nav class="navbar">

        <ul class="navbar-list">

          <li class="navbar-item">
            <button class="navbar-link  active" data-nav-link>About</button>
          </li>

          <li class="navbar-item">
            <button class="navbar-link" data-nav-link>Research</button>
          </li>

          <li class="navbar-item">
            <button class="navbar-link" data-nav-link>CV</button>
          </li>

          <li class="navbar-item">
            <button class="navbar-link" data-nav-link>Life</button>
          </li>
        </ul>

      </nav>

      <!--
        - #ABOUT
      -->

      <article class="about  active" data-page="about">

        <header>
          <h2 class="h2 article-title">About me</h2>
        </header>

        <section class="about-text">


          <p>

            <b class="text-bold">&#9733; I am currently looking for academic and industry positions.</b>

            Feel free to <a href="mailto:xiaoyingy@ucla.edu" class="text-link">reach out</a> if you are interested in
            chatting or collaborating. I am always open to new ideas and opportunities.

          </p>


          <p>

            I am a PhD candidate at the <a href="https://hilab.dev/" class="text-link">Human-Centered Computing &
              Intelligent Sensing Lab (HiLab)</a>,
            <a href="https://www.ee.ucla.edu/" class="text-link">Electrical and Computer Engineering (ECE)</a>,
            <a href="https://www.ucla.edu/" class="text-link">University of California, Los Angeles (UCLA)</a>,
            advised by <a href="https://yangzhang.dev/" class="text-link">Professor Yang Zhang</a>. My research lies at
            Human-Computer Interaction (HCI) and Sensing, focusing on <b>how
              energy and information flow during physical human-object interactions can enhance the context-awareness,
              connectivity and ubiquity of embedded computing systems</b>. I take a system-level research approach to
            creating
            innovative designs that make these systems low-cost, power-efficient, intelligent
            and easy to use, with the ultimate goal of seamlessly blending technology into everyday life.

          </p>

          <figure style="text-align: center; margin: 2rem 0;"></figure>
          <img src="./assets/images/research.png" alt="Research illustration"
            style="display: block; margin: 0 auto; width: 80%; border-radius: 12px; box-shadow: 0 2px 12px rgba(0,0,0,0.08);">

          </figure>
          <div style="height: 2rem;"></div>

          <p>

            I am a future technology imaginer, passionate about being part of researchers and engineers who are bringing
            J.A.R.V.I.S. (Just A Rather Very Intelligent System) closer to reality. My PhD training in HCI lets me go
            beyond daydreaming, turning ideas into tangible, functional prototypes by blending creativity, research, and
            engineering. My work often integrates hardware, electronics, computer vision and machine learning to deliver
            innovative solutions. I envision software intelligence and physical devices being developed jointly to truly
            support everyday tasks and enrich real-life experiences through enjoyable and meaningful interactions.

          </p>

        </section>


      </article>


      <article class="research" data-page="research">

        <header>
          <h2 class="h2 article-title" style="margin-bottom: 1.0rem;">Research</h2>
        </header>

        <p>
          My research projects have been published in top-tier HCI venues, including: <br>
          ACM Conference on Human Factors in Computing Systems (CHI)<br>
          ACM Symposium on User Interface Software and Technology (UIST)<br>
          The Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies (IMWUT)<br>

        </p>

        <section class="research-list">


          <div class="research-item">

            <div class="research-video-preview">
              <video src="./assets/images/luxact.mp4" autoplay muted loop playsinline></video>
            </div>

            <div class="research-info research-text">
              <p class="research-title"><b>
                  <rose>LuxAct: Enhance Everyday Objects for Visual Sensing with Interaction-Powered Illumination (UIST
                    2025)</rose>
                </b></p>

              <div class="research-tags">
                <span class="tag">augmented reality</span>
                <span class="tag">visible light communication</span>
                <span class="tag">computer vision</span>
              </div>

              <p class="research-authors"><u>Xiaoying Yang</u>, Qian Lu, Jeeeun Kim and Yang Zhang</p>


              <div class="research-links">


                <a href="javascript:void(0);" class="research-link expand-btn" onclick="toggleExtraInfo(this)">
                  <ion-icon name="chevron-down-outline"></ion-icon>
                  <span class="button-label">Technical Summary</span>
                </a>

                <a href="" class="research-link" target="_blank">
                  <ion-icon name="document-outline"></ion-icon> Paper (coming soon)
                </a>
                <a href="" class="research-link" target="_blank">
                  <ion-icon name="play-circle-outline"></ion-icon> Video (coming soon)
                </a>
              </div>


              <div class="extra-info">
                <ul>
                  <li>- Designed information encoding and decoding schemes for camera-LED communication</li>
                  <li>- Designed and implemented a real-time image processing pipeline in C++ (OpenCV), with
                    spatiotemporal feature engineering and graph-based LED tracking </li>
                  <li>- Engineered custom hardware modules across ten AR use cases</li>
                </ul>
              </div>

            </div>



          </div>

          <div class="research-item">

            <div class="research-video-preview">
              <video src="./assets/images/hapt-aids.mp4" autoplay muted loop playsinline></video>
            </div>

            <div class="research-info research-text">
              <p class="research-title"><b>
                  <rose>Hapt-Aids: Self-Powered, On-Body Haptics for Activity Monitoring (IMWUT 2025)</rose>
                </b></p>

              <div class="research-tags">
                <span class="tag">wearable sensing</span>
                <span class="tag">energy harvesting</span>
              </div>

              <p class="research-authors"><u>Xiaoying Yang<sup>&dagger;</sup></u>, Vivian Shen<sup>&dagger;</sup>, Chris
                Harrison and Yang Zhang (<sup>&dagger;</sup>Equal Contribution)</p>


              <div class="research-links">


                <a href="javascript:void(0);" class="research-link expand-btn" onclick="toggleExtraInfo(this)">
                  <ion-icon name="chevron-down-outline"></ion-icon>
                  <span class="button-label">Technical Summary</span>
                </a>

                <a href="" class="research-link" target="_blank">
                  <ion-icon name="document-outline"></ion-icon> Paper (coming soon)
                </a>
                <a href="" class="research-link" target="_blank">
                  <ion-icon name="play-circle-outline"></ion-icon> Video (coming soon)
                </a>
              </div>


              <div class="extra-info">
                <ul>
                  <li>- Designed and built custom analog circuits that convert harvested energy into haptics
                  </li>
                  <li>- Developed and validated four real-world applications across diverse body activities</li>
                </ul>
              </div>

            </div>

          </div>


          <div class="research-item">

            <div class="research-video-preview">
              <video src="./assets/images/powerstations.mp4" autoplay muted loop playsinline></video>
            </div>

            <div class="research-info research-text">
              <p class="research-title"><b>
                  <rose>Interaction-Power Stations: Turning Environments into Ubiquitous Power Stations for Charging
                    Wearables (CHI 2024, Late-Breaking Work)</rose>
                </b></p>

              <div class="research-tags">
                <span class="tag">energy harvesting</span>
                <span class="tag">wireless power transfer</span>
                <span class="tag">wearables</span>
              </div>

              <p class="research-authors"><u>Xiaoying Yang</u>, Jacob Sayono, Jess Xu and Yang Zhang</p>


              <div class="research-links">


                <a href="javascript:void(0);" class="research-link expand-btn" onclick="toggleExtraInfo(this)">
                  <ion-icon name="chevron-down-outline"></ion-icon>
                  <span class="button-label">Technical Summary</span>
                </a>

                <a href="https://dl.acm.org/doi/full/10.1145/3613905.3650769" class="research-link" target="_blank">
                  <ion-icon name="document-outline"></ion-icon> Paper
                </a>
                <a href="https://youtu.be/dBfFmrGLAQA?si=AA8F916RXB0vEZwi" class="research-link" target="_blank">
                  <ion-icon name="play-circle-outline"></ion-icon> Video
                </a>
              </div>


              <div class="extra-info">
                <ul>

                  <li>- Engineered hardware prototypes to harvest, modulate, and transmit energy through the
                    body to wirelessly charge wearables</li>
                  <li>- Investigated equivalent capacitive coupling model for wireless power transfer </li>
                  <li>- Built activity classification applications using power signals from interaction
                    events</li>
                </ul>
              </div>

            </div>



          </div>

          <div class="research-item">

            <div class="research-video-preview">
              <video src="./assets/images/headar.mp4" autoplay muted loop playsinline></video>
            </div>

            <div class="research-info research-text">
              <p class="research-title"><b>
                  <rose>Headar: Sensing Head Gestures for Confirmation Dialogs on Smartwatches with Wearable
                    Millimeter-Wave Radar (IMWUT 2023)</rose>
                </b></p>

              <div class="research-tags">
                <span class="tag">mmWave</span>
                <span class="tag">machine learning</span>
                <span class="tag">gesture sensing</span>
              </div>

              <p class="research-authors"><u>Xiaoying Yang</u>, Xue Wang, Gaofeng Dong, Zihan Yan, Mani Srivastava, Eiji
                Hayashi, Yang Zhang </p>

              <div class="research-links">



                <a href="javascript:void(0);" class="research-link expand-btn" onclick="toggleExtraInfo(this)">
                  <ion-icon name="chevron-down-outline"></ion-icon>
                  <span class="button-label">Technical Summary</span>
                </a>


                <a href="https://dl.acm.org/doi/10.1145/3610900" class="research-link" target="_blank">
                  <ion-icon name="document-outline"></ion-icon> Paper
                </a>
                <a href="https://www.youtube.com/watch?v=haQjXGVPx94" class="research-link" target="_blank">
                  <ion-icon name="play-circle-outline"></ion-icon> Video
                </a>
              </div>

              <div class="extra-info">
                <ul>
                  <li>- Conducted signal simulation and analysis to extract head gesture features in MATLAB</li>
                  <li>- Characterized individual gesture features through OptiTrack motion capture</li>
                  <li>- Developed a real-time mmWave+IMU signal acquisition and processing pipeline in Python</li>
                  <li>- Trained a spatiotemporal, multimodal deep learning model to recognize head gestures</li>
                  <li>- Built an application with Wear OS for smartwatch-laptop data communication</li>
                </ul>
              </div>
            </div>

          </div>



          <div class="research-item">

            <div class="research-video-preview">
              <video src="./assets/images/cubesense+.mp4" autoplay muted loop playsinline></video>
            </div>

            <div class="research-info research-text">
              <p class="research-title"><b>
                  <rose>CubeSense++: Smart Environment Sensing with Interaction-Powered Corner Reflector Mechanisms
                    (UIST 2023)
                  </rose>
                </b></p>

              <div class="research-tags">
                <span class="tag">mmWave backscatter</span>
                <span class="tag">battery-free</span>
                <span class="tag">activity sensing</span>
              </div>

              <p class="research-authors"><u>Xiaoying Yang</u>, Jacob Sayono and Yang Zhang</p>

              <div class="research-links">


                <a href="javascript:void(0);" class="research-link expand-btn" onclick="toggleExtraInfo(this)">
                  <ion-icon name="chevron-down-outline"></ion-icon>
                  <span class="button-label">Technical Summary</span>
                </a>


                <a href="https://dl.acm.org/doi/10.1145/3586183.3606744" class="research-link" target="_blank">
                  <ion-icon name="document-outline"></ion-icon> Paper
                </a>

                <a href="https://youtu.be/D3uQFWdRM4I?si=xfAlhIn6S5dF0-Sn" class="research-link" target="_blank">
                  <ion-icon name="play-circle-outline"></ion-icon> Video
                </a>

              </div>

              <div class="extra-info">
                <ul>
                  <li>- Designed reflectors using optimization techniques to uniquely respond to
                    interactions</li>

                  <li>- Developed a real-time signal processing pipeline in Python for object and activity
                    recognition
                  </li>
                </ul>
              </div>

            </div>

          </div>



          <div class="research-item">

            <div class="research-video-preview">
              <video src="./assets/images/minikers.mp4" autoplay muted loop playsinline></video>
            </div>

            <div class="research-info research-text">
              <p class="research-title"><b>
                  <rose>MiniKers: Interaction-Powered Smart Environment Automation (IMWUT 2022)</rose>
                </b></p>

              <div class="research-tags">
                <span class="tag">energy harvesting</span>
                <span class="tag">home automation</span>

              </div>

              <p class="research-authors"><u>Xiaoying Yang</u>, Jacob Sayono, Jess Xu, Jiahao "Nick" Li, Josiah Hester
                and
                Yang Zhang</p>
              <div class="research-links">

                <a href="javascript:void(0);" class="research-link expand-btn" onclick="toggleExtraInfo(this)">
                  <ion-icon name="chevron-down-outline"></ion-icon>
                  <span class="button-label">Technical Summary</span>
                </a>


                <a href="https://dl.acm.org/doi/10.1145/3550287" class="research-link" target="_blank">
                  <ion-icon name="document-outline"></ion-icon> Paper
                </a>
              </div>

              <div class="extra-info">
                <ul>
                  <li>- Designed a custom energy management circuit that integrates energy harvesting, sensing, and
                    actuation using the nRF52</li>
                  <li>- Characterized power signal profiles from user interactions for activity sensing in Python</li>
                </ul>
              </div>

            </div>

          </div>



          <div class="research-item">

            <div class="research-video-preview">
              <video src="./assets/images/cubesense.mp4" autoplay muted loop playsinline></video>
            </div>

            <div class="research-info research-text">
              <p class="research-title"><b>
                  <rose>CubeSense: Wireless, Battery-Free Interactivity through Low-Cost Corner Reflector Mechanisms
                    (CHI 2021, Late-Breaking Work)
                  </rose>
                </b></p>

              <div class="research-tags">
                <span class="tag">mmWave backscatter</span>
                <span class="tag">battery-free input interfaces</span>
              </div>

              <p class="research-authors"><u>Xiaoying Yang</u> and Yang Zhang</p>

              <div class="research-links">

                <a href="https://dl.acm.org/doi/10.1145/3411763.3451599" class="research-link" target="_blank">
                  <ion-icon name="document-outline"></ion-icon> Paper
                </a>
                <a href="https://youtu.be/_-9qkTDiPBQ?si=03DidjllB8cZfaEH" class="research-link" target="_blank">
                  <ion-icon name="play-circle-outline"></ion-icon> Video
                </a>
              </div>

            </div>

          </div>






        </section>

        <style>
          .research-list {
            margin-bottom: 2.5rem;
          }
        </style>

        <header>
          <h3 class="h3 article-title">Other Research</h3>
        </header>

        <section class="research-list">


          <div class="research-item">

            <div class="research-video-preview">
              <video src="./assets/images/lumosX.mp4" autoplay muted loop playsinline></video>
            </div>

            <div class="research-info research-text">
              <p class="research-title"><b>
                  <rose>LumosX: 3D Printed Anisotropic Light-Transfer (CHI 2025)</rose>
                </b></p>

              <div class="research-tags">
                <span class="tag">fabrication</span>
                <span class="tag">battery-free</span>
                <span class="tag">sensing</span>
              </div>

              <p class="research-authors">Qian Lu, <u>Xiaoying Yang</u>, Xue Wang, Jacob Sayono, Yang Zhang, and Jeeeun
                Kim </p>

              <div class="research-links">
                <a href="https://dl.acm.org/doi/full/10.1145/3706598.3714124" class="research-link" target="_blank">
                  <ion-icon name="document-outline"></ion-icon> Paper
                </a>
                <a href="./assets/videos/lumosX.mp4" class="research-link" target="_blank">
                  <ion-icon name="play-circle-outline"></ion-icon> Video
                </a>
              </div>

            </div>

          </div>


          <div class="research-item">

            <div class="research-video-preview">
              <video src="./assets/images/forcesight.mp4" autoplay muted loop playsinline></video>
            </div>

            <div class="research-info research-text">
              <p class="research-title"><b>
                  <rose>ForceSight: Non-Contact Force Sensing with Laser Speckle Imaging (UIST 2022)</rose>
                </b></p>

              <div class="research-tags">
                <span class="tag">computer vision</span>
                <span class="tag">gesture sensing</span>
                <span class="tag" title="Honorable Mention" style="display: inline-flex; align-items: center;">
                  <span style="font-size: 1em; margin-right: 0.2em;">üèÖ</span> Honorable Mention for Demo
                </span>
              </div>

              <p class="research-authors">Siyou Pei, Pradyumna Chari, Xue Wang, <u>Xiaoying Yang</u>, Achuta Kadambi,
                Yang Zhang</p>

              <div class="research-links">
                <a href="https://dl.acm.org/doi/10.1145/3526113.3545622" class="research-link" target="_blank">
                  <ion-icon name="document-outline"></ion-icon> Paper
                </a>
                <a href="https://youtu.be/ErxOZNqMSAc?si=MHlTx3tnrLv9fep8" class="research-link" target="_blank">
                  <ion-icon name="play-circle-outline"></ion-icon> Video
                </a>
              </div>
            </div>

          </div>




        </section>

        <style>
          .research-list {
            margin-bottom: 2.5rem;
          }
        </style>

        <header>
          <h3 class="h3 article-title">Industrial Projects</h3>
        </header>

        <section class="research-list">



          <div class="research-item">

            <div class="research-video-preview">
              <video src="./assets/images/ofdm.mp4" autoplay muted loop playsinline></video>
            </div>

            <div class="research-info research-text">
              <p class="research-title"><b>
                  <rose>Vibration Sensing with Communication-Centric mmWave (2024)</rose>
                </b></p>

              <div class="research-tags">
                <span class="tag">joint communication and sensing</span>
                <span class="tag">machine learning</span>
              </div>

              <!-- <p class="research-venue">Manuscript in preparation</p> -->

              <div class="research-links">

                <a href="javascript:void(0);" class="research-link expand-btn" onclick="toggleExtraInfo(this)">
                  <ion-icon name="chevron-down-outline"></ion-icon>
                  <span class="button-label">Technical Summary</span>
                </a>


                <a href="https://youtu.be/makPwTt6NgY?si=DnMVQDz1IGh4d4-p" class="research-link" target="_blank">
                  <ion-icon name="play-circle-outline"></ion-icon> Video
                </a>
              </div>

              <div class="extra-info">
                <ul>
                  <li>- Investigated signal denoising and synthesis through target and channel feature disentanglement
                  </li>
                  <li>- Implemented data collection and signal processing pipelines in C#</li>
                  <li>- Designed and trained machine learning models for sensing applications using PyTorch</li>
                  <li>- Deployed pre-trained models for real-time inference using ONNX</li>
                </ul>
              </div>


            </div>

          </div>


          <div class="research-item">

            <div class="research-video-preview">
              <video src="./assets/images/carla.mp4" autoplay muted loop playsinline></video>
            </div>

            <div class="research-info research-text">
              <p class="research-title"><b>
                  <rose>Investigation of Model Predictive Control Strategy for Autonomous Vehicles (2019)</rose>
                </b></p>

              <div class="research-tags">
                <span class="tag">autonomous driving</span>
                <span class="tag">control</span>
              </div>

              <!-- <p class="research-venue">Hands-on industrial project</p> -->

              <div class="research-links">
                <a href="javascript:void(0);" class="research-link expand-btn" onclick="toggleExtraInfo(this)">
                  <ion-icon name="chevron-down-outline"></ion-icon>
                  <span class="button-label">Technical Summary</span>
                </a>
              </div>


              <div class="extra-info">
                <ul>
                  <li>- Built and tested model predictive control and sliding mode control for trajectory tracking using
                    Python and CARLA </li>
                  <li>- Integrated control algorithms into autonomous car codebase running on Linux</li>
                </ul>
              </div>



            </div>

          </div>

          <div class="research-item">

            <div class="research-video-preview">
              <video src="./assets/images/naovideo.mp4" autoplay muted loop playsinline></video>
            </div>

            <div class="research-info research-text">
              <p class="research-title"><b>
                  <rose>NAO Robot Kicking Ball (2019)</rose>
                </b></p>

              <div class="research-tags">
                <span class="tag">robotics</span>
                <span class="tag">computer vision</span>
                <span class="tag">object detection</span>
              </div>

              <!-- <p class="research-venue">Hands-on industrial project</p> -->

              <div class="research-links">
                <a href="javascript:void(0);" class="research-link expand-btn" onclick="toggleExtraInfo(this)">
                  <ion-icon name="chevron-down-outline"></ion-icon>
                  <span class="button-label">Technical Summary</span>
                </a>
              </div>


              <div class="extra-info">
                <ul>
                  <li>- Implemented speech recognition, object detection and localization in Python </li>
                </ul>
              </div>




            </div>

          </div>



        </section>

      </article>

      <article class="cv" data-page="cv">
        <header>
          <h2 class="h2 article-title">CV</h2>
        </header>

        <section class="about-text" style="display: flex; justify-content: center;">
          <iframe src="/assets/images/XiaoyingCV.pdf" width="80%" height="800px"
            style="border: none; border-radius: 16px; overflow: hidden;">
          </iframe>
        </section>
      </article>

      <!-- Life  -->

      <article class="life" data-page="life">

        <header>
          <h2 class="h2 article-title">My "HCI"</h2>
        </header>

        <section class="about-text">
          <p>
            I am an HCI researcher by day, aiming to creat technologies that shape a better future.
            By night, I switch to a different kind of HCI ‚Äî Human-Cat Interaction,
            where my furry supervisors set the agenda.
          </p>
          <p>
            They are key contributors to both the breakthroughs and the breakdowns of my research,
            snoring their opinions into my zoom meetings,
            shutting down my computer with a single paw while I am on Overleaf,
            waking me up at 5am to make sure I never miss the AOE deadline,
            and chewing through my breadboard jumper wires for quality control. True collaborators.
          </p>
        </section>

        <ul class="project-list">

          <li class="project-item active" data-filter-item data-category="video">
            <div class="gif-hover">
              <img class="static" src="./assets/images/cat_1.jpg" alt="Preview">
              <img class="animated" src="./assets/images/cat_1.gif" alt="Animated">
            </div>
          </li>

          <li class="project-item active" data-filter-item data-category="video">
            <div class="gif-hover">
              <img class="static" src="./assets/images/cat_3.jpg" alt="Preview">
              <img class="animated" src="./assets/images/cat_3.gif" alt="Animated">
            </div>
          </li>

          <li class="project-item active" data-filter-item data-category="video">
            <div class="gif-hover">
              <img class="static" src="./assets/images/cat_2.jpg" alt="Preview">
              <img class="animated" src="./assets/images/cat_2.gif" alt="Animated">
            </div>
          </li>

        </ul>

      </article>

    </div>

  </main>






  <!--
    - custom js link
  -->
  <script src="./assets/js/script.js"></script>

  <!--
    - ionicon link
  -->
  <script type="module" src="https://unpkg.com/ionicons@5.5.2/dist/ionicons/ionicons.esm.js"></script>
  <script nomodule src="https://unpkg.com/ionicons@5.5.2/dist/ionicons/ionicons.js"></script>

  <script>
    function toggleExtraInfo(button) {
      const infoBox = button.closest('.research-links').nextElementSibling;
      const isOpen = infoBox.classList.contains("open");

      if (isOpen) {
        infoBox.classList.remove("open");
        button.querySelector('.button-label').textContent = "Technical Summary";
      } else {
        infoBox.classList.add("open");
        button.querySelector('.button-label').textContent = "Hide Summary";
      }
    }
  </script>

</body>

</html>